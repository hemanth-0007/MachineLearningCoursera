{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Optional Lab - Softmax Function\n",
    "In this lab, we will explore the softmax function. This function is used in both Softmax Regression and in Neural Networks when solving Multiclass Classification problems.  \n",
    "\n",
    "<center>  <img  src=\"./images/C2_W2_Softmax_Header.PNG\" width=\"600\" />  <center/>\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from sklearn.datasets import make_blobs\n",
    "%matplotlib widget\n",
    "from matplotlib.widgets import Slider\n",
    "from lab_utils_common import dlc\n",
    "from lab_utils_softmax import plt_softmax\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Normally, in this course, the notebooks use the convention of starting counts with 0 and ending with N-1,  $\\sum_{i=0}^{N-1}$, while lectures start with 1 and end with N,  $\\sum_{i=1}^{N}$. This is because code will typically start iteration with 0 while in lecture, counting 1 to N leads to cleaner, more succinct equations. This notebook has more equations than is typical for a lab and thus  will break with the convention and will count 1 to N."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Softmax Function\n",
    "In both softmax regression and neural networks with Softmax outputs, N outputs are generated and one output is selected as the predicted category. In both cases a vector $\\mathbf{z}$ is generated by a linear function which is applied to a softmax function. The softmax function converts $\\mathbf{z}$  into a probability distribution as described below. After applying softmax, each output will be between 0 and 1 and the outputs will add to 1, so that they can be interpreted as probabilities. The larger inputs  will correspond to larger output probabilities.\n",
    "<center>  <img  src=\"./images/C2_W2_SoftmaxReg_NN.png\" width=\"600\" />  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function can be written:\n",
    "$$a_j = \\frac{e^{z_j}}{ \\sum_{k=1}^{N}{e^{z_k} }} \\tag{1}$$\n",
    "The output $\\mathbf{a}$ is a vector of length N, so for softmax regression, you could also write:\n",
    "\\begin{align}\n",
    "\\mathbf{a}(x) =\n",
    "\\begin{bmatrix}\n",
    "P(y = 1 | \\mathbf{x}; \\mathbf{w},b) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = N | \\mathbf{x}; \\mathbf{w},b)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{ \\sum_{k=1}^{N}{e^{z_k} }}\n",
    "\\begin{bmatrix}\n",
    "e^{z_1} \\\\\n",
    "\\vdots \\\\\n",
    "e^{z_{N}} \\\\\n",
    "\\end{bmatrix} \\tag{2}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which shows the output is a vector of probabilities. The first entry is the probability the input is the first category given the input $\\mathbf{x}$ and parameters $\\mathbf{w}$ and $\\mathbf{b}$.  \n",
    "Let's create a NumPy implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(z):\n",
    "    ez = np.exp(z)              #element-wise exponenial\n",
    "    sm = ez/np.sum(ez)\n",
    "    return(sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, vary the values of the `z` inputs using the sliders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79df608de6e84f9597f8c34896c19e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close(\"all\")\n",
    "plt_softmax(my_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you are varying the values of the z's above, there are a few things to note:\n",
    "* the exponential in the numerator of the softmax magnifies small differences in the values \n",
    "* the output values sum to one\n",
    "* the softmax spans all of the outputs. A change in `z0` for example will change the values of `a0`-`a3`. Compare this to other activations such as ReLU or Sigmoid which have a single input and single output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cost\n",
    "<center> <img  src=\"./images/C2_W2_SoftMaxCost.png\" width=\"400\" />    <center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function associated with Softmax, the cross-entropy loss, is:\n",
    "\\begin{equation}\n",
    "  L(\\mathbf{a},y)=\\begin{cases}\n",
    "    -log(a_1), & \\text{if $y=1$}.\\\\\n",
    "        &\\vdots\\\\\n",
    "     -log(a_N), & \\text{if $y=N$}\n",
    "  \\end{cases} \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "Where y is the target category for this example and $\\mathbf{a}$ is the output of a softmax function. In particular, the values in $\\mathbf{a}$ are probabilities that sum to one.\n",
    ">**Recall:** In this course, Loss is for one example while Cost covers all examples. \n",
    " \n",
    " \n",
    "Note in (3) above, only the line that corresponds to the target contributes to the loss, other lines are zero. To write the cost equation we need an 'indicator function' that will be 1 when the index matches the target and zero otherwise. \n",
    "    $$\\mathbf{1}\\{y == n\\} = =\\begin{cases}\n",
    "    1, & \\text{if $y==n$}.\\\\\n",
    "    0, & \\text{otherwise}.\n",
    "  \\end{cases}$$\n",
    "Now the cost is:\n",
    "\\begin{align}\n",
    "J(\\mathbf{w},b) = -\\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{N}  1\\left\\{y^{(i)} == j\\right\\} \\log \\frac{e^{z^{(i)}_j}}{\\sum_{k=1}^N e^{z^{(i)}_k} }\\right] \\tag{4}\n",
    "\\end{align}\n",
    "\n",
    "Where $m$ is the number of examples, $N$ is the number of outputs. This is the average of all the losses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "This lab will discuss two ways of implementing the softmax, cross-entropy loss in Tensorflow, the 'obvious' method and the 'preferred' method. The former is the most straightforward while the latter is more numerically stable.\n",
    "\n",
    "Let's start by creating a dataset to train a multiclass classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make  dataset for example\n",
    "centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\n",
    "X_train, y_train = make_blobs(n_samples=2000, centers=centers, cluster_std=1.0,random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the X_train is  (2000, 2)\n",
      "Shape of the y_train is  (2000,)\n",
      "[1.56 0.85] 2\n",
      "[-5.34  1.03] 0\n",
      "[-4.09  0.68] 0\n",
      "[-1.36 -1.5 ] 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the X_train is \", X_train.shape)\n",
    "# print(X_train[0])\n",
    "print(\"Shape of the y_train is \", y_train.shape)\n",
    "print(X_train[0], y_train[0])\n",
    "print(X_train[1], y_train[1])\n",
    "print(X_train[2], y_train[2])\n",
    "print(X_train[3], y_train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3f7c8ed1c740c4bd9fc00cb235934d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', marker='o', edgecolors=\"k\")\n",
    "\n",
    "# Plot the centers\n",
    "# for center in centers:\n",
    "#     plt.scatter(center[0], center[1], s=200, c='red', marker='X')\n",
    "\n",
    "for i in range(len(centers)):\n",
    "    plt.scatter(centers[i][0], centers[i][1], s=200, c='red', marker='X')\n",
    "        \n",
    "# Set plot title and labels\n",
    "plt.title('Scatter Plot of Generated Blobs')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The *Obvious* organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model below is implemented with the softmax as an activation in the final Dense layer.\n",
    "The loss function is separately specified in the `compile` directive. \n",
    "\n",
    "The loss function is `SparseCategoricalCrossentropy`. This loss is described in (3) above. In this model, the softmax takes place in the last layer. The loss function takes in the softmax output which is a vector of probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.7156\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2773\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1354\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0887\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0684\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0573\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0506\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0454\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0418\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0384\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 0s 981us/step - loss: 0.0363\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0337\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0318\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0300\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0286\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0272\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0259\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0245\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0232\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0224\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0218\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0206\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0201\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0193\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0190\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0183\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0177\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0173\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0174\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0165\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0162\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 0s 957us/step - loss: 0.0159\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0157\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0156\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0151\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0152\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0144\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0143\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0144\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0141\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0139\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0135\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0134\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0136\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0135\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0136\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0131\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0128\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0128\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0124\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0128\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0127\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0130\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0129\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0121\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 0s 988us/step - loss: 0.0124\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0122\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0119\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0123\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0117\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0118\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0117\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0113\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0123\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0117\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0115\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0115\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0114\n",
      "Epoch 69/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0116\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0110\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0115\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0111\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0109\n",
      "Epoch 74/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0115\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0111\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0115\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0113\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0110\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0111\n",
      "Epoch 80/100\n",
      "63/63 [==============================] - 0s 998us/step - loss: 0.0110\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0108\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0105\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0106\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0107\n",
      "Epoch 85/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0112\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0103\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0106\n",
      "Epoch 91/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0106\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0102\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0101\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0100\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0102\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 0s 996us/step - loss: 0.0104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7d72845ad450>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential(\n",
    "    [ \n",
    "        Dense(25, activation = 'relu'),\n",
    "        Dense(15, activation = 'relu'),\n",
    "        Dense(4, activation = 'softmax')    # < softmax activation here\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=100\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the softmax is integrated into the output layer, the output is a vector of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.09e-09 3.00e-06 1.00e+00 7.55e-06]\n",
      " [1.00e+00 1.93e-06 6.49e-10 3.29e-10]]\n",
      "largest value 1.0 smallest value 2.8358995e-21\n"
     ]
    }
   ],
   "source": [
    "p_nonpreferred = model.predict(X_train)\n",
    "print(p_nonpreferred [:2])\n",
    "print(\"largest value\", np.max(p_nonpreferred), \"smallest value\", np.min(p_nonpreferred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.61e-01 8.72e-03 3.00e-02 2.98e-06]]\n",
      "largest value 0.96130455 smallest value 2.9792077e-06\n",
      "[[9.61e-01 8.72e-03 3.00e-02 2.98e-06]], category: 0\n"
     ]
    }
   ],
   "source": [
    "p_nonpreferred = model.predict([[-2, 2.034]])\n",
    "print(p_nonpreferred)\n",
    "print(\"largest value\", np.max(p_nonpreferred), \"smallest value\", np.min(p_nonpreferred))\n",
    "print( f\"{p_nonpreferred}, category: {np.argmax(p_nonpreferred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preferred <img align=\"Right\" src=\"./images/C2_W2_softmax_accurate.png\"  style=\" width:400px; padding: 10px 20px ; \">\n",
    "Recall from lecture, more stable and accurate results can be obtained if the softmax and loss are combined during training.   This is enabled by the 'preferred' organization shown here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preferred organization the final layer has a linear activation. For historical reasons, the outputs in this form are referred to as *logits*. The loss function has an additional argument: `from_logits = True`. This informs the loss function that the softmax operation should be included in the loss calculation. This allows for an optimized implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.1153\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4931\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2345\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1361\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0978\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0791\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0688\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0617\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0567\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7d731444eb90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preferred_model = Sequential(\n",
    "    [ \n",
    "        Dense(25, activation = 'relu'),\n",
    "        Dense(15, activation = 'relu'),\n",
    "        Dense(4, activation = 'linear')   #<-- Note\n",
    "    ]\n",
    ")\n",
    "preferred_model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #<-- Note\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "preferred_model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=10\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Handling\n",
    "Notice that in the preferred model, the outputs are not probabilities, but can range from large negative numbers to large positive numbers. The output must be sent through a softmax when performing a prediction that expects a probability. \n",
    "Let's look at the preferred model outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two example output vectors:\n",
      " [[-1.43 -3.13  3.67  0.67]\n",
      " [ 4.17 -0.32 -5.37 -4.6 ]]\n",
      "largest value 10.64781 smallest value -9.269767\n"
     ]
    }
   ],
   "source": [
    "p_preferred = preferred_model.predict(X_train)\n",
    "print(f\"two example output vectors:\\n {p_preferred[:2]}\")\n",
    "print(\"largest value\", np.max(p_preferred), \"smallest value\", np.min(p_preferred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output predictions are not probabilities!\n",
    "If the desired output are probabilities, the output should be be processed by a [softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two example output vectors:\n",
      " [[5.76e-03 1.05e-03 9.46e-01 4.70e-02]\n",
      " [9.89e-01 1.11e-02 7.11e-05 1.53e-04]]\n",
      "largest value 0.9999945 smallest value 2.8565312e-09\n"
     ]
    }
   ],
   "source": [
    "sm_preferred = tf.nn.softmax(p_preferred).numpy()\n",
    "print(f\"two example output vectors:\\n {sm_preferred[:2]}\")\n",
    "print(\"largest value\", np.max(sm_preferred), \"smallest value\", np.min(sm_preferred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the most likely category, the softmax is not required. One can find the index of the largest output using [np.argmax()](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.43 -3.13  3.67  0.67], category: 2\n",
      "[ 4.17 -0.32 -5.37 -4.6 ], category: 0\n",
      "[ 3.14  0.1  -4.21 -3.64], category: 0\n",
      "[-0.53  3.92 -3.77 -0.87], category: 1\n",
      "[-0.12 -5.76  4.99 -0.91], category: 2\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print( f\"{p_preferred[i]}, category: {np.argmax(p_preferred[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparseCategorialCrossentropy or CategoricalCrossEntropy\n",
    "Tensorflow has two potential formats for target values and the selection of the loss defines which is expected.\n",
    "- SparseCategorialCrossentropy: expects the target to be an integer corresponding to the index. For example, if there are 10 potential target values, y would be between 0 and 9. \n",
    "- CategoricalCrossEntropy: Expects the target value of an example to be one-hot encoded where the value at the target index is 1 while the other N-1 entries are zero. An example with 10 potential target values, where the target is 2 would be [0,0,1,0,0,0,0,0,0,0].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "In this lab you \n",
    "- Became more familiar with the softmax function and its use in softmax regression and in softmax activations in neural networks. \n",
    "- Learned the preferred model construction in Tensorflow:\n",
    "    - No activation on the final layer (same as linear activation)\n",
    "    - SparseCategoricalCrossentropy loss function\n",
    "    - use from_logits=True\n",
    "- Recognized that unlike ReLU and Sigmoid, the softmax spans multiple outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
